base_model: "medalpaca/medalpaca-7b"

# Training hyperparameters
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 2
  num_train_epochs: 6
  learning_rate: 2e-4
  optim: "paged_adamw_32bit"
  fp16: true
  output_dir: "/workspace/trained_models/medalpaca/"

# LoRA / model-specific parameters
model:
  use_lora: true
  lora_rank: 128
  lora_dropout: 0.1
  load_quantized: null
